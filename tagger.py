import torch
import timm
import onnxruntime
import numpy as np
import torchvision.transforms.functional as TVF
from PIL import Image
from pathlib import Path
from typing import List
import csv
import json
import logging

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger("uvicorn")

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° ---
MODEL_PATH = Path('./models_data')
SAFETENSORS_MODEL_FILE = MODEL_PATH / 'model.safetensors'
ONNX_MODEL_FILE = MODEL_PATH / 'model.onnx'
CONFIG_FILE = MODEL_PATH / 'config.json'
TAG_FILE = MODEL_PATH / 'selected_tags.csv'

THRESHOLD = 0.35
# ONNX Runtimeã§CUDAãŒä½¿ãˆã‚‹ã‹ãƒã‚§ãƒƒã‚¯
DEVICE = 'cuda' if 'CUDAExecutionProvider' in onnxruntime.get_available_providers() else 'cpu'

# --- ONNXãƒ¢ãƒ‡ãƒ«ãŒãªã‘ã‚Œã°è‡ªå‹•ã§å¤‰æ›ã™ã‚‹é–¢æ•° ---
def ensure_onnx_model_exists():
    """
    ONNXãƒ¢ãƒ‡ãƒ«ãŒãªã‹ã£ãŸã‚‰ã€safetensorsã‹ã‚‰è‡ªå‹•ã§ä½œã£ã¡ã‚ƒã†é­”æ³•ã®é–¢æ•°
    """
    if ONNX_MODEL_FILE.exists():
        logger.info(f"âœ… `{ONNX_MODEL_FILE}` ç™ºè¦‹ï¼å¤‰æ›ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã­ã€‚")
        return

    logger.warning(f"âš ï¸ `{ONNX_MODEL_FILE}` ãŒè¦‹ã¤ã‹ã‚‰ãªã„ï¼ä»Šã‹ã‚‰safetensorsã‹ã‚‰å¤‰æ›ã™ã‚‹ã‚ˆã€‚ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ã­â€¦")

    if not SAFETENSORS_MODEL_FILE.exists():
        logger.error(f"âŒ ã†ãâ€¦è‚å¿ƒã® `{SAFETENSORS_MODEL_FILE}` ã‚‚ãªã„ã˜ã‚ƒã‚“ï¼ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã¦ï¼")
        raise FileNotFoundError(f"{SAFETENSORS_MODEL_FILE} not found.")
        
    try:
        with open(CONFIG_FILE, 'r') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ `{CONFIG_FILE}` ãŒãªã„ã‚ˆï¼ãƒ¢ãƒ‡ãƒ«ã¨ä¸€ç·’ã«é…å¸ƒã•ã‚Œã¦ã‚‹ã¯ãšã ã‹ã‚‰ç¢ºèªã—ã¦ï¼")
        raise
        
    logger.info("ãƒ¢ãƒ‡ãƒ«ã®éª¨æ ¼ã‚’timmã§ä½œæˆä¸­...")
    model = timm.create_model(
        'eva02_large_patch14_448.mim_in22k_ft_in22k_in1k',
        pretrained=False,
        num_classes=config['n_tags']
    )

    logger.info("safetensorsã‹ã‚‰é­‚ï¼ˆãŠã‚‚ã¿ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    from safetensors.torch import load_file
    state_dict = load_file(SAFETENSORS_MODEL_FILE, device='cpu')
    model.load_state_dict(state_dict)
    model.eval()
    
    image_size = config.get('image_size', 448)
    dummy_input = torch.randn(1, 3, image_size, image_size, requires_grad=True)

    logger.info("âœ¨ ONNXã«å¤‰èº«é–‹å§‹ï¼ã“ã‚ŒãŒçµæ§‹æ™‚é–“ã‹ã‹ã‚‹ã‹ã‚‚â€¦ âœ¨")
    
    # ONNXã¸ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    torch.onnx.export(
        model,
        dummy_input,
        str(ONNX_MODEL_FILE),
        export_params=True,
        opset_version=14,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={'input' : {0 : 'batch_size'},
                      'output' : {0 : 'batch_size'}}
    )
    logger.info(f"ğŸ‰ çˆ†é€ŸONNXãƒ¢ãƒ‡ãƒ« `{ONNX_MODEL_FILE}` ã®ä½œæˆå®Œäº†ï¼ ğŸ‰")


# --- ã‚¿ã‚°ãƒªã‚¹ãƒˆã¨ç¿»è¨³è¾æ›¸ã®ä½œæˆ ---
tags_list = []
translation_map = {}
if TAG_FILE.exists():
    with open(TAG_FILE, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        header = next(reader)
        try:
            name_index = header.index('name')
            japanese_name_index = header.index('japanese_name')
        except ValueError:
            japanese_name_index = -1
            logger.warning("âš ï¸ 'japanese_name'åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‹ã‚‰ã€ç¿»è¨³æ©Ÿèƒ½ã¯ã‚ªãƒ•ã«ãªã‚‹ã‚ˆã€‚")

        for row in reader:
            try:
                english_tag = row[name_index].replace('_', ' ')
                tags_list.append(english_tag)
                if japanese_name_index != -1 and row[japanese_name_index]:
                    translation_map[english_tag] = row[japanese_name_index]
            except IndexError:
                continue
    logger.info("âœ… ã‚¿ã‚°ãƒªã‚¹ãƒˆã¨ç¿»è¨³è¾æ›¸ã®èª­ã¿è¾¼ã¿å®Œäº†ï¼")
else:
    logger.error(f"âŒ {TAG_FILE} ãŒè¦‹ã¤ã‹ã‚‰ãªã„ï¼å‡¦ç†ã‚’ä¸­æ–­ã™ã‚‹ã­ã€‚")
    exit()


# â˜…â˜…â˜… ã“ã“ã‹ã‚‰ãŒæœ¬ç•ªï¼ â˜…â˜…â˜…

# 1. ã¾ãšONNXãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã—ã¦ã€ãªã‹ã£ãŸã‚‰ä½œã‚‹ï¼
ensure_onnx_model_exists()

# 2. ONNXãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
logger.info(f"Loading ONNX model from {ONNX_MODEL_FILE} on {DEVICE}...")
providers = ['CUDAExecutionProvider'] if DEVICE == 'cuda' else ['CPUExecutionProvider']
ort_session = onnxruntime.InferenceSession(str(ONNX_MODEL_FILE), providers=providers)
logger.info("âœ… ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ã„ã¤ã§ã‚‚æ¨è«–OKï¼")


# --- ç”»åƒã®å‰å‡¦ç†é–¢æ•° ---
def prepare_image(image: Image.Image, target_size: int) -> np.ndarray:
    # ç”»åƒã‚’æ­£æ–¹å½¢ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    image_shape = image.size
    max_dim = max(image_shape)
    pad_left = (max_dim - image_shape[0]) // 2
    pad_top = (max_dim - image_shape[1]) // 2

    padded_image = Image.new('RGB', (max_dim, max_dim), (255, 255, 255))
    padded_image.paste(image, (pad_left, pad_top))

    # ãƒªã‚µã‚¤ã‚º
    if max_dim != target_size:
        padded_image = padded_image.resize((target_size, target_size), Image.BICUBIC)

    # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã€æ­£è¦åŒ–
    image_tensor = TVF.to_tensor(padded_image).unsqueeze(0)
    image_tensor = TVF.normalize(image_tensor, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])

    return image_tensor.numpy()


# --- ã‚¿ã‚°äºˆæ¸¬é–¢æ•° ---
def predict(image: Image.Image) -> List[str]:
    # ONNXãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã‚µã‚¤ã‚ºã‚’å–å¾—
    input_size = ort_session.get_inputs()[0].shape[-1]
    image_np = prepare_image(image.convert("RGB"), input_size)

    # ONNX Runtimeã§æ¨è«–
    input_name = ort_session.get_inputs()[0].name
    output_name = ort_session.get_outputs()[0].name
    
    ort_inputs = {input_name: image_np}
    ort_outs = ort_session.run([output_name], ort_inputs)[0]

    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ç¢ºç‡ã«å¤‰æ›
    preds = 1 / (1 + np.exp(-ort_outs))
    tag_preds = preds[0]

    # ã‚¹ã‚³ã‚¢ã¨ã‚¿ã‚°åã‚’ç´ã¥ã‘
    scores = {tags_list[i]: tag_preds[i] for i in range(len(tags_list))}

    # ã‚¹ã‚³ã‚¢ãŒé–¾å€¤ã‚’è¶…ãˆãŸè‹±èªã‚¿ã‚°ã ã‘ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
    predicted_tags = [tag for tag, score in scores.items() if score > THRESHOLD]

    # ç¿»è¨³è¾æ›¸ã‚’ä½¿ã£ã¦æ—¥æœ¬èªã«å¤‰æ›
    translated_tags = [translation_map.get(tag, tag) for tag in predicted_tags]

    return translated_tags